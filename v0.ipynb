{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c55c96-14f7-47ba-b243-74810b410cf4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from pydataquery import DataQuery\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from xbbg import blp\n",
    "import numpy as np\n",
    "import pytz\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import statsmodels.api as sm\n",
    "from ipywidgets import interact, Dropdown, HBox, VBox, Button, Output, Text, widgets\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import re\n",
    "import scipy.stats as stats\n",
    "from adjustText import adjust_text\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import sympy as sp\n",
    "import itertools\n",
    "import warnings\n",
    "import openpyxl\n",
    "import subprocess\n",
    "import time\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import pyodbc\n",
    "import ast\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec475d55-89fc-490f-9f7e-11205368f40f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df_original = pd.read_parquet(\"Markit CDS.parquet\")\n",
    "    \n",
    "    to_date = datetime.today().date()\n",
    "    from_date = df_original[\"close_date\"].iloc[-1].date()\n",
    "    \n",
    "    from_date_str = from_date.strftime('%m/%d/%Y')\n",
    "    to_date_str = to_date.strftime('%m/%d/%Y')\n",
    "    \n",
    "    conn_str = (\n",
    "        f'DRIVER={{SQL Server}};'\n",
    "        f'SERVER=BC-ODS-P1;'\n",
    "        f'DATABASE=MarkitDB;'\n",
    "        f'ApplicationIntent=ReadOnly;'\n",
    "        f'Trusted_Connection=Yes;'\n",
    "        f'Authentication=ActiveDirectoryIntegrated;'\n",
    "    )\n",
    "    \n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    \n",
    "    query1 = f\"\"\"\n",
    "        DECLARE @FromDate DATE = '{from_date_str}';\n",
    "        DECLARE @ToDate DATE = '{to_date_str}';\n",
    "        \n",
    "        SELECT sc.close_date, r.ticker, c.red, c.tier, c.docclause, c.ccy, sc.tenor, sc.spread\n",
    "        FROM dbo.RedEntities r\n",
    "        INNER JOIN dbo.MarkitCurves c ON r.red = c.red\n",
    "        INNER JOIN dbo.MarkitSpreadCurve sc ON c.curve_id = sc.curve_id\n",
    "        WHERE sc.close_date >= @FromDate AND sc.close_date <= @ToDate;\n",
    "    \"\"\"\n",
    "    \n",
    "    df_new = pd.read_sql(query1, conn)\n",
    "    conn.close()\n",
    "\n",
    "    df_old = df_original[df_original[\"close_date\"]<pd.to_datetime(df_original[\"close_date\"].iloc[-1])]\n",
    "\n",
    "    df1 = pd.concat([df_old, df_new])\n",
    "    df1 = df1.drop_duplicates()\n",
    "    df1.to_parquet(\"Markit CDS.parquet\")\n",
    "except:\n",
    "    df1 = pd.read_parquet(\"Markit CDS.parquet\")\n",
    "    hello = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e07c3361-551e-4aa6-979b-5edb794f1bad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "markit_cds = list(set(df1[\"red\"]))\n",
    "markit_df1 = df1.copy()\n",
    "\n",
    "res_codes = { \"Full Restructuring\": \"CR14\", \"Modified Restructuring\": \"MR14\",\n",
    "          \"Modified-Modified Restructurin\": \"MM14\", \"No Restructuring\": \"XR14\"}\n",
    "\n",
    "excel_df = None\n",
    "all_dq = None\n",
    "all_temp_cds = None\n",
    "\n",
    "for rating_col in[\"IG\",\"HY\",\"EUR IG\",\"EUR HY\",\"SNRFIN\"]:\n",
    "    dq = pd.read_excel(\"CDX Members.xlsx\", sheet_name=rating_col)\n",
    "    dq[\"Restructuring\"] = dq[\"Restructuring\"].apply(lambda x: res_codes[x])\n",
    "    dq = dq[dq[\"Actual RED Code\"].isin(markit_cds)].reset_index(drop=True).copy()\n",
    "    dq.rename(columns={\"Actual RED Code\": \"red\"}, inplace=True)\n",
    "    dq[\"Eqty\"] = dq[\"Issuer Equity\"]\n",
    "\n",
    "    all_dq = pd.concat([all_dq, dq]).drop_duplicates().reset_index(drop=True).copy()\n",
    "    \n",
    "    #################################\n",
    "    df1 = markit_df1.copy()\n",
    "    cds_df1 = df1[(df1[\"tier\"]==\"SNRFOR\") & (df1[\"ccy\"]==\"USD\") & (df1[\"tenor\"]!=\"Spot\") &\\\n",
    "        (df1[\"docclause\"].str.endswith(\"14\"))].drop([\"tier\",\"ccy\"],axis=1).copy()\n",
    "    \n",
    "    #################################\n",
    "    cds_df = cds_df1.copy()\n",
    "    \n",
    "    df = pd.merge(left=cds_df, right=dq, on=\"red\", how=\"inner\")\n",
    "    df = df[df[\"docclause\"]==df[\"Restructuring\"]].drop([\"RED Code\",\"Restructuring\"],axis=1).reset_index(drop=True).copy()\n",
    "    cds_df = df[[\"close_date\",\"ticker\",\"red\",\"tenor\",\"spread\",\"Eqty\",\"Bond Name\"]].copy()\n",
    "    cds_df.rename(columns={\"Eqty\": \"Eqty Name\"}, inplace=True)\n",
    "    \n",
    "    cds_df[\"tenor\"] = cds_df[\"tenor\"].apply(lambda x: eval(x.replace(\"y\",\"*1\").replace(\"m\",\"*(1/12)\")))\n",
    "    cds_df = cds_df[~pd.isna(cds_df[\"Eqty Name\"])]\n",
    "    cds_df = cds_df[~(cds_df[\"Eqty Name\"].str.startswith(\"#\"))].reset_index(drop=True).copy()\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "    #############################################################\n",
    "    issuers = list(sorted(set(cds_df[\"Eqty Name\"])))\n",
    "    last_dt = max(cds_df[\"close_date\"])\n",
    "    \n",
    "    all_curves = None\n",
    "    \n",
    "    try:\n",
    "        for issuer in issuers: ################################################################## issuers instead of issuers1 when going live\n",
    "            df = cds_df[cds_df[\"Eqty Name\"]==issuer]\n",
    "            df = pd.pivot_table(df, values=\"spread\", index=\"close_date\", columns =\"tenor\")\n",
    "            \n",
    "            if not last_dt in df.index:\n",
    "                df.loc[last_dt] = [np.nan] * len(df.columns)\n",
    "            df = df.sort_index().ffill().copy()\n",
    "            \n",
    "            df[f\"{issuer}_curve\"] = [np.nan] * len(df)\n",
    "        \n",
    "            for idx in df.index:\n",
    "                # idx = df.index[0]\n",
    "                curve = df.loc[[idx],:].dropna(axis=1)\n",
    "                curve = curve.iloc[:,:-1].copy()\n",
    "                x = list((curve.columns))\n",
    "                x2 = [item**2 for item in x]\n",
    "                X = np.column_stack([x, x2])\n",
    "                Y = list(curve.iloc[0])\n",
    "        \n",
    "                if len(X) > 2:\n",
    "                    model = sm.OLS(Y, sm.add_constant(X)).fit()\n",
    "                    df.loc[idx,f\"{issuer}_curve\"] = str([model.params[0], model.params[1], model.params[2]])\n",
    "                    \n",
    "                    # x_pred = list(np.linspace(min(x), max(x),100))\n",
    "                    # x_pred2 = [item**2 for item in x_pred]\n",
    "                    # X_pred = np.column_stack([x_pred, x_pred2])\n",
    "                    # y_pred = model.predict(sm.add_constant(X_pred))\n",
    "                    \n",
    "                    # plt.plot(x, Y)\n",
    "                    # plt.plot(x_pred, y_pred)\n",
    "                    # title = f\"{issuer} on {str(idx.date())}\"\n",
    "                    # plt.title(title)\n",
    "                    # # plt.savefig(f\"Curve Plots v2/{title}.png\")\n",
    "                    # plt.show()\n",
    "                    # plt.close()\n",
    "                    \n",
    "                else:\n",
    "                    df.loc[idx,f\"{issuer}_curve\"] = str([np.nan, np.nan, np.nan])\n",
    "        \n",
    "            all_curves = pd.concat([all_curves,df.iloc[:,[-1]]],axis=1)\n",
    "        # all_curves.to_excel(\"All CDS Curves.xlsx\")\n",
    "    except:\n",
    "        # all_curves = pd.read_excel(\"All CDS Curves.xlsx\",index_col=0, parse_dates=True)\n",
    "        hello=1\n",
    "        \n",
    "    all_curves1 = all_curves.copy()\n",
    "    all_curves = all_curves.T.copy()\n",
    "    all_curves.index = [item.split(\"_\")[0] for item in all_curves.index]\n",
    "    all_curves.index.name = \"Eqty Name\"\n",
    "    all_curves.columns = [f'CDS_{item.date()}' for item in all_curves.columns]\n",
    "    all_curves = all_curves.reset_index(drop=False).copy()\n",
    "    temp_cds = all_curves.copy()\n",
    "    all_temp_cds = pd.concat([all_temp_cds, temp_cds]).drop_duplicates().reset_index(drop=True).copy()\n",
    "\n",
    "################################################################################################################################\n",
    "    # bql_list = []\n",
    "    \n",
    "    # fl = [f\"\"\"=BQL(\"filter(bonds(['\"\"\", f\"\"\" Equity']), payment_rank=='Sr Unsecured' AND crncy=='USD')\", \"id_isin, id_cusip, name, maturity, amt_outstanding\")\"\"\"]\n",
    "    # for item in issuers:\n",
    "    #     bql_list += [(fl[0] + item + \" \" + fl[1])]\n",
    "        \n",
    "    # bql_list = [item.replace(\"\\\\\", \"\") for item in bql_list]\n",
    "    \n",
    "    # workbook = openpyxl.load_workbook(r\"J:\\\\HY Basis Data.xlsx\")\n",
    "    # sheet = workbook.active\n",
    "    \n",
    "    # for row in sheet.iter_rows():\n",
    "    #     for cell in row:\n",
    "    #         cell.value = None\n",
    "    \n",
    "    # start_col = 1\n",
    "    # for item in bql_list:\n",
    "    #     cell = sheet.cell(row=2, column=start_col)\n",
    "    #     cell.value = item\n",
    "    #     start_col += 6\n",
    "    # workbook.save(r\"J:\\\\HY Basis Data.xlsx\")\n",
    "    \n",
    "    # file_path = r\"J:\\\\HY Basis Data.xlsx\"\n",
    "    # window_title = \"HY Basis Data - Excel\"\n",
    "    \n",
    "    # subprocess.Popen(['start', 'excel', file_path], shell=True)\n",
    "    # time.sleep(5)\n",
    "    \n",
    "    # excel_windows = [window for window in gw.getWindowsWithTitle('Excel')]\n",
    "    \n",
    "    # for window in excel_windows:\n",
    "    #     if window_title in window.title:\n",
    "    #         # time.sleep(0.5)\n",
    "    #         window.activate()\n",
    "    #         break\n",
    "    \n",
    "    # time.sleep(45)\n",
    "    # pyautogui.hotkey('ctrl', 's')\n",
    "    # time.sleep(1)\n",
    "    # pyautogui.hotkey('alt', 'f4')\n",
    "    \n",
    "    # # for window in excel_windows:\n",
    "    # #     if window_title in window.title:\n",
    "    # #         # time.sleep(0.5)\n",
    "    # #         window.activate()\n",
    "    # #         break\n",
    "    \n",
    "    # # pyautogui.hotkey('alt', 'f4')\n",
    "    # time.sleep(1.5)\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "    issuer_dict = dict(zip(dq[\"Issuer Equity\"],[item.split(\" \")[0] for item in dq[\"Bond Name\"]]))\n",
    "    \n",
    "    ############################################################################ Now get those bonds\n",
    "    \n",
    "    hy = pd.read_excel(\"All Bonds.xlsx\",sheet_name=rating_col)\n",
    "    all_df = None\n",
    "    \n",
    "    # dq2 = globals()[f'{rating_col}_dq'].copy()\n",
    "    # issuer_dict = dict(zip(dq2[\"Issuer Equity\"],[item.split(\" \")[0] for item in dq2[\"Bond Name\"]]))\n",
    "    \n",
    "    for i in range(len(hy.columns))[::6]:\n",
    "        x = hy.iloc[:,i:i+6].dropna().copy()\n",
    "        if len(x) > 0:\n",
    "            x.columns = ['ID','ISIN', \"CUSIP\", 'Name', 'Maturity','Amt']\n",
    "            x[\"Eqty Name\"] = [issuer_dict[issuers[(int(i/6))]]] * len(x)\n",
    "            x[\"Check Col\"] = x.apply(lambda row: row[\"Name\"].split(\" \")[0]==row[\"Eqty Name\"],axis=1)\n",
    "            x = x[x[\"Check Col\"]].drop(\"Check Col\",axis=1)\n",
    "            all_df = pd.concat([all_df, x])\n",
    "    \n",
    "    all_df['Time'] = round(((pd.to_datetime(all_df['Maturity'])-datetime.now()).dt.days/365),2)\n",
    "    all_df = all_df[all_df[\"Time\"]>=0]\n",
    "    all_df = all_df[(all_df['Time']>=4) & (all_df['Time']<=10)]\n",
    "    all_df = all_df[all_df['Amt']>=300*10**6]\n",
    "    all_df = all_df.reset_index(drop=True)\n",
    "    excel_df = pd.concat([excel_df, all_df])\n",
    "all_df = excel_df.copy()\n",
    "\n",
    "all_df = all_df.drop_duplicates().reset_index(drop=True).copy()\n",
    "################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc4ccf1d-e05c-4c59-ad83-60c46d00d4a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "###################################### 144A and REGS\n",
    "\n",
    "blist = [f'/isin/{item}@BGN' for item in list(all_df[\"ISIN\"])]\n",
    "blist = blp.bdp(tickers=blist, flds=[\"144A_FLAG\",\"IS_REG_S\"])\n",
    "blist.index = [item.rsplit(\"/\",1)[1].split(\"@\")[0] for item in blist.index]\n",
    "\n",
    "blist.columns = [\"144A\",\"REGS\"]\n",
    "blist.index.name = \"ISIN\"\n",
    "blist = blist.reset_index()\n",
    "blist[\"REGS_144A\"] = blist.apply(lambda row: f'{row[\"REGS\"]}_{row[\"144A\"]}',axis=1)\n",
    "order = [\"N_N\", \"Y_N\", \"Y_Y\", \"N_Y\"]\n",
    "\n",
    "all_df = pd.merge(left=all_df, right=blist, on=\"ISIN\", how=\"outer\")\n",
    "all_df = all_df[[item for item in all_df.columns if not item in [\"144A\",\"REGS\"]]]\n",
    "\n",
    "\n",
    "df2 = all_df.copy()\n",
    "df2[\"REGS_144A\"] = pd.Categorical(df2[\"REGS_144A\"], categories=order, ordered=True)\n",
    "df2 = df2.sort_values(by=\"REGS_144A\")\n",
    "df2 = df2[~df2[\"Name\"].duplicated(keep='first')].drop([\"REGS_144A\",\"Time\"],axis=1).reset_index(drop=True).copy()\n",
    "# df2.to_excel(\"HY Bonds List.xlsx\")\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "# df2 = pd.read_excel(\"HY Bonds List.xlsx\", index_col=0, parse_dates=True)\n",
    "\n",
    "t = [f\"/isin/{item}@BGN\" for item in list(df2[\"ISIN\"])]\n",
    "bbg = blp.bdh(tickers=t, flds=\"BLOOMBERG_MID_G_SPREAD\", start_date=datetime.now()-timedelta(days=365*5))\n",
    "bbg.to_parquet(\"Test1.parquet\")\n",
    "bbg = pd.read_parquet(\"Test1.parquet\")\n",
    "\n",
    "bbg1 = bbg.copy()\n",
    "new = []\n",
    "for item in bbg1.columns:\n",
    "    new += [\"BBG_\" + item[0].replace(\"/isin/\",\"\").replace(\"@BGN\",\"\")]\n",
    "bbg1.columns = new\n",
    "bbg1.index = pd.to_datetime(bbg1.index)\n",
    "\n",
    "########################### choose bbg or dq sprds\n",
    "bbg1.columns = [item.split(\"_\")[1] for item in bbg1.columns]\n",
    "bbg1.index = [f\"Sprd_{str(item.date())}\" for item in bbg1.index]\n",
    "bbg1 = bbg1.T.copy()\n",
    "bbg1.index.name = \"ISIN\"\n",
    "bbg1 = bbg1.reset_index(drop=False).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a43a5e3-a23e-47cb-ae42-62e7d54c694a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "for dt in pd.to_datetime(bbg.index):\n",
    "    df3[f'Mat_Time_{dt.date()}'] = [((pd.to_datetime(item) - dt).days/365) for item in df3[\"Maturity\"]]\n",
    "\n",
    "red2 = dict(zip([item.split(\" \")[0] for item in all_dq[\"Bond Name\"]], all_dq[\"Eqty\"]))\n",
    "df3[\"Eqty Name\"] = df3[\"Eqty Name\"].apply(lambda x: red2[x])\n",
    "\n",
    "######################################################################\n",
    "\n",
    "df4 = pd.merge(left = df3, right = all_temp_cds, on=\"Eqty Name\", how=\"outer\")\n",
    "df4 = df4[~pd.isna(df4[\"ISIN\"])].reset_index(drop=True).copy()\n",
    "dt_list = [item.replace(\"CDS_\",\"\") for item in df4.columns if item.startswith(\"CDS_\")]\n",
    "\n",
    "df4A = df4.copy()\n",
    "\n",
    "for dt in dt_list:\n",
    "    if f\"Mat_Time_{dt}\" in df4A.columns and f\"CDS_{dt}\" in df4A.columns:\n",
    "        df4A[f'Mat_Matched_CDS_{dt}'] = df4A[f'CDS_{dt}'].apply(lambda x: ast.\\\n",
    "            literal_eval(x)[0] if isinstance(x,str) else np.nan) +\\\n",
    "        df4A[f'Mat_Time_{dt}'].apply(lambda x: x) *\\\n",
    "        df4A[f'CDS_{dt}'].apply(lambda x: ast.literal_eval(x)[1] if isinstance(x,str) else np.nan) +\\\n",
    "        df4A[f'Mat_Time_{dt}'].apply(lambda x: x**2) *\\\n",
    "        df4A[f'CDS_{dt}'].apply(lambda x: ast.literal_eval(x)[2] if isinstance(x,str) else np.nan)\n",
    "      \n",
    "df4A = df4A[[col for col in df4A.columns if not \"_\" in col] +\\\n",
    "    [col for col in df4A.columns if col.startswith(\"Mat_Matched_CDS_\")]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "810b9e5b-6ccc-4821-bac2-c72f2c44c4e0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df5 = pd.merge(left=df4A, right=bbg1, how='outer', on=\"ISIN\")\n",
    "last_update = max(dt_list)\n",
    "for dt in dt_list:\n",
    "    try:\n",
    "        df5[f\"Basis_{dt}\"] = df5[f\"Mat_Matched_CDS_{dt}\"] - df5[f\"Sprd_{dt}\"]\n",
    "    except:\n",
    "        hello = 1\n",
    "l1 = list(set([col for col in df5.columns if f'{last_update}' in col] + [col for col in df5.columns if 'Basis' in col] ))\n",
    "l2 = [item for item in l1 if f'{last_update}' in item] + [item for item in l1 if not f'{last_update}' in item]\n",
    "df5 = df5[[col for col in df5.columns if not \"_\" in col] + l2]\n",
    "df5 = df5[~pd.isna(df5[f\"Sprd_{last_update}\"])].reset_index(drop=True).copy()\n",
    "\n",
    "red = all_dq[[\"Eqty\",\"red\"]].dropna().copy()\n",
    "# red_eqty_dict1 = dict(zip([item.split(\" \")[0] for item in list(red[\"Eqty\"])], list(red[\"red\"])))\n",
    "red_eqty_dict1 = dict(zip(list(red[\"Eqty\"]), list(red[\"red\"])))\n",
    "\n",
    "df5[\"CDS RED Code\"] = df5[\"Eqty Name\"].apply(lambda x: red_eqty_dict1[x])\n",
    "\n",
    "####################################### Zscore and 1Y High/Low Calc\n",
    "\n",
    "dfz = df5[[\"ISIN\"] + [item for item in df5.columns if \"Basis\" in item]]\n",
    "dfz = dfz.set_index(\"ISIN\").T\n",
    "dfz.index = [pd.to_datetime(item.split(\"_\")[1]).date() for item in dfz.index]\n",
    "dfz = dfz.sort_index()\n",
    "\n",
    "all_z_df = None\n",
    "for period in [3*22, 6*22, 12*22]:\n",
    "    x = ((dfz-dfz.rolling(period, min_periods = int(0.6*period)).mean())/dfz.rolling(period,\\\n",
    "                  min_periods = int(0.6*period)).std()).iloc[[-1],:].copy()\n",
    "    x.index = [f'ZScore_{int(period/22)}M']\n",
    "    all_z_df = pd.concat([all_z_df , x])\n",
    "\n",
    "max_1y = pd.DataFrame(dfz.iloc[-12*22:,:].max()).T\n",
    "max_1y.index = [f'Basis 1Y High']\n",
    "min_1y = pd.DataFrame(dfz.iloc[-12*22:,:].min()).T\n",
    "min_1y.index = [f'Basis 1Y Low']\n",
    "\n",
    "all_z_df = pd.concat([all_z_df , max_1y, min_1y])\n",
    "all_z_df = all_z_df.T\n",
    "all_z_df = all_z_df.reset_index()\n",
    "\n",
    "df6 = pd.merge(left=df5[[col for col in df5.columns if not col.startswith(\"Basis_\") or dt in col]],\n",
    "               right=all_z_df, on=\"ISIN\", how=\"outer\")\n",
    "df6 = df6[[\"Name\", \"ISIN\", \"Eqty Name\", \"CDS RED Code\", f\"Basis_{last_update}\",\n",
    "           f\"Sprd_{last_update}\", f\"Mat_Matched_CDS_{last_update}\"] + list(df6.columns)[-5:]]\n",
    "\n",
    "\n",
    "################################### Add PX\n",
    "px_list = [f\"/isin/{i}@BGN\" for i in list(df6[\"ISIN\"])]\n",
    "px = blp.bdp(tickers=px_list, flds=\"px_last\")\n",
    "px.to_parquet(\"Test2.parquet\")\n",
    "px = pd.read_parquet(\"Test2.parquet\")\n",
    "\n",
    "px1 = px.copy()\n",
    "px1.index = [item.replace(\"/isin/\",\"\").replace(\"@BGN\",\"\") for item in px1.index]\n",
    "px1.columns = [\"Price\"]\n",
    "px1.index.name =\"ISIN\"\n",
    "px1 = px1.reset_index().copy()\n",
    "df6 = pd.merge(left=df6, right=px1, how=\"outer\", on=\"ISIN\")\n",
    "\n",
    "\n",
    "################################################## Adding IG/HY\n",
    "blist = [f\"/isin/{item}@BGN\" for item in list(df6[\"ISIN\"])]\n",
    "ig_hy = blp.bdp(tickers=blist, flds=\"BB_COMPSTE_RATING_IG_HY_INDCTR\")\n",
    "ig_hy.to_parquet(\"Test3.parquet\")\n",
    "ig_hy = pd.read_parquet(\"Test3.parquet\")\n",
    "\n",
    "ig_hy[\"ISIN\"] = [item.replace(\"/isin/\",\"\").replace(\"@BGN\",\"\") for item in ig_hy.index]\n",
    "ig_hy.columns = [\"IG_HY\",\"ISIN\"]\n",
    "ig_hy = ig_hy.reset_index(drop=True).copy()\n",
    "df6 = pd.merge(left=df6, right=ig_hy, how=\"outer\", on=\"ISIN\")\n",
    "df6 = df6.sort_values(by=\"Eqty Name\").reset_index(drop=True).copy()\n",
    "\n",
    "################################################## \n",
    "\n",
    "df7 = df6.copy()\n",
    "df7[\"Time\"] = df7[\"Name\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "df7 = df7.sort_values(by=[\"Eqty Name\",\"Time\"],ascending=True).\\\n",
    "    reset_index(drop=True).drop([\"Eqty Name\",\"Time\",\"ISIN\"],axis=1).copy()\n",
    "df7 = df7.set_index(\"Name\")\n",
    "df7.columns = df7.columns.str.replace(f'_{last_update}',\"\")\n",
    "df7.index.name = f'{pd.to_datetime(last_update).strftime(\"%d-%b\")}'\n",
    "df7 = round(df7,2)\n",
    "df7 = df7[[\"IG_HY\",\"CDS RED Code\",\"Price\",\"Sprd\",\"Mat_Matched_CDS\",\"Basis\",\n",
    "           \"Basis 1Y Low\",\"Basis 1Y High\",\"ZScore_3M\",\"ZScore_6M\",\"ZScore_12M\"]]\n",
    "\n",
    "df7[\"Name\"] = [item.split(\" \")[0] for item in df7.index]\n",
    "df7[\"Year\"] = [eval(item.split(\"/\")[-1]) for item in df7.index]\n",
    "df7 = df7.sort_values(by=[\"Name\",\"Year\"], ascending=True).drop([\"Name\",\"Year\"],axis=1).copy()\n",
    "\n",
    "px_new = blp.bdh(tickers=blist, flds=\"PX_LAST\", start_date = datetime.now()-timedelta(days=5*365))\n",
    "px_new.columns = [item.replace(\"/isin/\",\"\").replace(\"@BGN\",\"\") + \"_Price\" for item in blist]\n",
    "px_new.to_parquet(\"Test4.parquet\")\n",
    "px_new = pd.read_parquet(\"Test4.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc3ebaaa-f6c5-4273-aaf6-fea8422e27b9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# dq_bonds = ['US00130HCC79','US00130HCG83','US00206RCP55','US00206RMM15','US012873AH83','US012873AK13','US013092AE14','US01309QAB41','US023135AP19','US023551AF16','US023551AJ38','US023551AM66','US026874DC84','US031162DQ06','US03674XAS53','US03743QAF54','US03743QAQ10','US05368VAA44','US05368VAB27','US058498AW66','US058498AX40','US058498BA38','US097023AU94','US097023CJ22','US097023CN34','US097023CP81','US097023CY98','US097023DC69','US097023DR39','US097023DS12','US11135FAS02','US11135FBD24','US11135FBF71','US11135FBH38','US11135FBK66','US11135FBL40','US11135FBT75','US126650DJ69','US126650DU15','US126650ED80','US12769GAA85','US12769GAD25','US134429BJ73','US149123CH22','US165167DH73','US166756AS52','US185899AL57','US185899AN14','US185899AP61','US185899AQ45','US185899AR28','US185899AS01','US23331ABT51','US23918KAS78','US23918KAT51','US23918KAW80','US23918KAY47','US247361A329','US247361ZT81','US251799AA02','US25179MBF95','US25179SAD27','US254709AS70','US26884LAG41','US26884LAN91','US26884LAR06','US26884LBA61','US26884LBC28','US292505AD65','US30212PAR64','US30212PBH73','US30231GBN16','US337932AL12','US337932AP26','US345370CA64','US345370CX67','US345370DA55','US345370DB39','US35671DBJ37','US35671DCD57','US35671DCF06','US35671DCH61','US364760AP35','US364760AQ18','US36962GXZ26','US370425RZ53','US37045VAH33','US37045VAY65','US37045VAZ31','US382550BJ95','US382550BK68','US382550BR12','US382550BS94','US404119CA57','US404119CC14','US404119CK30','US404119CQ00','US404119CT49','US404119DB22','US404121AK12','US437076CB65','US44106MAY84','US44106MBB72','US458140BR09','US46284VAF85','US46284VAJ08','US46284VAL53','US46284VAN10','US46284VAQ41','US48666KAY55','US48666KAZ21','US48666KBA60','US500255AX28','US501797AW48','US513272AD65','US513272AE49','US526057CY87','US55262CAJ99','US552676AT59','US552676AU23','US552953CJ87','US552953CK50','US55616XAM92','US55617LAR33','US55617LAS16','US571903BE27','US571903BF91','US571903BG74','US571903BH57','US626717AP72','US629377CR16','US629377CS98','US629377CW01','US629377CX83','US62957HAP01','US62957HAQ83','US63861CAD11','US63861CAE93','US63861CAF68','US63938CAN83','US63938CAP32','US63938CAQ15','US64110LAU08','US64110LAV80','US651229BD74','US651229BE57','US651229BF23','US674599DD43','US674599DE26','US674599EA94','US674599ED34','US674599EF81','US674599EK76','US674599EL59','US677347CH71','US680665AK27','US680665AN65','US68389XBV64','US68389XCE31','US68389XCH61','US68389XCJ28','US68622FAB76','US68622TAB70','US69047QAC69','US698900AG20','US737446AP91','US737446AQ74','US737446AR57','US737446AV69','US737446AX26','US745867AM30','US745867AP60','US745867AT82','US75513ECR09','US780153BU54','US780153BV38','US780153BW11','US78442FAZ18','US785931AA40','US785931AB23','US78646UAA79','US78646UAB52','US81211KAK60','US812127AB45','US812127AC28','US81761LAE20','US845467AS85','US845467AT68','US87264ABF12','US87264ABT16','US87264ABW45','US87264ABX28','US87264ACB98','US87264ACQ67','US87264ACV52','US87264ADT97','US87612GAA94','US87901JAH86','US88033GAV23','US88947EAU47','US893830AF64','US893830BZ10','US902494AZ66','US90353TAK60','US90353TAN00','US90353TAP57','US925524AH30','US925524AV24','US92556HAB33','US92556HAD98','US963320AY28','US963320AZ92','US963320BA33','US963320BC98','US963320BD71','US963320BE54','US988498AL59','US988498AN16','US988498AP63','US988498AR20']\n",
    "\n",
    "dq_bonds = list(df5[\"ISIN\"])\n",
    "all_labels = dict(zip(df5[\"ISIN\"],[f\"DB(CREDIT,HY,BOND,{item},MDUR)\" for item in df5[\"CUSIP\"]]))\n",
    "\n",
    "labels = {}\n",
    "for item in dq_bonds:\n",
    "    labels[f\"{item}_Dur\"] = all_labels[item]\n",
    "\n",
    "try:\n",
    "    # df1 = pd.read_excel(\"DQ HY Duration Data.xlsx\",index_col=0, parse_dates=True)\n",
    "    dq = DataQuery(\n",
    "        client_id='jbAIMF2Tkp0JO3sc',\n",
    "        client_secret='d7qfzgt55pddjs352sgxosFyI4t2eja07k7opbi6wg9oqjc1OjkdAksn1btmnugeMjchcx2vwTsJupw',\n",
    "        calendar = 'CAL_USBANK',\n",
    "    )\n",
    "    \n",
    "    job = dq.create_job(expressions = list(labels.values()))\n",
    "    dq.start_date = str((datetime.now()-timedelta(days=5*365)).date())\n",
    "    job.execute()\n",
    "    df = job.to_pivot_table()\n",
    "    df = df.T\n",
    "    df.index.name = 'Date'\n",
    "    df.index = pd.to_datetime(df.index, format='%Y%m%d').date\n",
    "    df.columns.name = None\n",
    "    \n",
    "    df1 = pd.DataFrame()\n",
    "    for key in labels:\n",
    "        df1[key] = df[labels[key]]\n",
    "    \n",
    "    df1 = df1[list(labels.keys())].copy()\n",
    "    clear_output(wait=False)\n",
    "    df1.dropna(axis=1, how='all', inplace=True)\n",
    "    df1.to_excel(\"DQ HY Duration Data.xlsx\")\n",
    "except:\n",
    "    df1 = pd.read_excel(\"DQ HY Duration Data.xlsx\",index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22626355-4832-44e6-ad5b-57fec8147700",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A = df4A[[\"ISIN\"]+[col for col in df4A.columns if col.startswith(\"Mat_Matched_CDS_\")]].set_index(\"ISIN\")\n",
    "A = A.T\n",
    "A.index = pd.to_datetime(A.index.str.replace(\"Mat_Matched_CDS_\",\"\"))\n",
    "A.columns = [f\"{item}_Mat_Matched_CDS_\" for item in A.columns]\n",
    "\n",
    "B = df5[[\"ISIN\"]+[col for col in df5.columns if col.startswith(\"Basis_\")]].set_index(\"ISIN\")\n",
    "B = B.T\n",
    "B.index = pd.to_datetime(B.index.str.replace(\"Basis_\",\"\"))\n",
    "B.columns = [f\"{item}_Basis\" for item in B.columns]\n",
    "\n",
    "C = bbg1[[\"ISIN\"]+[col for col in bbg1.columns if col.startswith(\"Sprd_\")]].set_index(\"ISIN\")\n",
    "C = C.T\n",
    "C.index = pd.to_datetime(C.index.str.replace(\"Sprd_\",\"\"))\n",
    "C.columns = [f\"{item}_G-Sprd\" for item in C.columns]\n",
    "\n",
    "D = px_new.copy()\n",
    "\n",
    "E = df1.copy()\n",
    "# E = E.set_index(\"close_date\")\n",
    "\n",
    "for item in [\"A\",\"B\",\"C\",\"D\",\"E\"]:\n",
    "    globals()[f\"{item}\"].index = pd.to_datetime(globals()[f\"{item}\"].index)\n",
    "\n",
    "F = pd.concat([A,B,C,D,E], axis=1).sort_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e9beb7-bc30-48b0-843e-fc36648386a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ######################### HY bonds TR data from BBG using BQL\n",
    "\n",
    "# import openpyxl\n",
    "# import subprocess\n",
    "# import time\n",
    "# import pyautogui\n",
    "# import pygetwindow as gw\n",
    "    \n",
    "# if True:\n",
    "#     bql_list = []\n",
    "    \n",
    "#     fl = [f'=BQL(\"', f\"\"\"ISIN\", \"return_series(calc_interval=range(-5y,0d,frq=d))\")\"\"\"]\n",
    "    \n",
    "#     issuers = df6[\"ISIN\"].to_list()\n",
    "    \n",
    "#     for item in issuers:\n",
    "#         bql_list += [(fl[0] + item + \" \" + fl[1])]\n",
    "        \n",
    "#     bql_list = [item.replace(\"\\\\\", \"\") for item in bql_list]\n",
    "    \n",
    "#     workbook = openpyxl.load_workbook(r\"J:\\\\55 BQL HY Bonds Rtn Data.xlsx\")\n",
    "#     sheet = workbook.active\n",
    "    \n",
    "#     for row in sheet.iter_rows():\n",
    "#         for cell in row:\n",
    "#             cell.value = None\n",
    "    \n",
    "#     start_col = 1\n",
    "#     for item in bql_list:\n",
    "#         cell = sheet.cell(row=2, column=start_col)\n",
    "#         cell.value = item\n",
    "#         start_col += 2\n",
    "#     workbook.save(r\"J:\\\\55 BQL HY Bonds Rtn Data.xlsx\")\n",
    "    \n",
    "#     file_path = r\"J:\\\\55 BQL HY Bonds Rtn Data.xlsx\"\n",
    "#     window_title = \"55 BQL HY Bonds Rtn Data - Excel\"\n",
    "    \n",
    "#     subprocess.Popen(['start', 'excel', file_path], shell=True)\n",
    "#     time.sleep(40)\n",
    "    \n",
    "#     excel_windows = [window for window in gw.getWindowsWithTitle('Excel')]\n",
    "    \n",
    "#     for check in range(2):\n",
    "#         for window in excel_windows:\n",
    "#             if window_title in window.title:\n",
    "#                 # time.sleep(0.25)\n",
    "#                 window.activate()\n",
    "#                 pyautogui.hotkey('ctrl', 's')\n",
    "#                 time.sleep(1)\n",
    "#                 pyautogui.hotkey('alt', 'f4')\n",
    "#                 # break\n",
    "    \n",
    "# time.sleep(0.5)\n",
    "\n",
    "# hy = pd.read_excel(r\"J:\\\\55 BQL HY Bonds Rtn Data.xlsx\",skiprows=1,parse_dates=True)\n",
    "# hy = hy.iloc[1:,:]\n",
    "# hy = hy[[hy.columns[0]] + [col for col in hy.columns if \"ISIN\" in col]]\n",
    "# hy.columns = [\"Date\"] + [item.replace(\" ISIN\",\"_Daily_Rtn\") for item in list(hy.columns)[1:]]\n",
    "# hy = hy.set_index(\"Date\")\n",
    "# hy.index = pd.to_datetime(hy.index)\n",
    "# hy = (1+hy).cumprod()\n",
    "# hy_col = hy.columns\n",
    "# for col in hy_col:\n",
    "#     hy[f'{col.split(\"_\")[0]} FWD_1M_Rtn'] = (hy[col].shift(-21) / hy[col] - 1) * 100\n",
    "#     hy[f'{col.split(\"_\")[0]} FWD_3M_Rtn'] = (hy[col].shift(-63) / hy[col] - 1) * 100\n",
    "#     hy[f'{col.split(\"_\")[0]} FWD_6M_Rtn'] = (hy[col].shift(-126) / hy[col] - 1) * 100\n",
    "# hy = hy[[col for col in hy.columns if \"FWD\" in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dae2dc39-d4d0-4c56-90f8-4a0f426fd117",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.concat([F, hy],axis=1).sort_index().copy()\n",
    "\n",
    "df = F.sort_index().copy()\n",
    "df.columns = df.columns.str.replace(\"_Mat_Matched_CDS_\",\" Mat. Matched CDS\").\\\n",
    "    str.replace(\"_Basis\",\" Basis\").str.replace(\"_Dur\",\" Duration\").\\\n",
    "    str.replace(\"_Price\",\" Price\").str.replace(\"_G-Sprd\",\" G-Sprd\")\n",
    "\n",
    "bond_dict = dict(zip(list(df2[\"ISIN\"]),list(df2[\"Name\"])))\n",
    "df.columns = [f\"{bond_dict[item.split(\" \",1)[0]]} {item.split(\" \",1)[1]}\"  for item in df.columns]\n",
    "df = df.dropna(how=\"all\").copy()\n",
    "df.columns = [f\"{item.split(\"/\",1)[0].rsplit(\" \",1)[0]}% due {item.\\\n",
    "    rsplit(\"/\",1)[1]}\" for item in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f519a4ae-33eb-4bd5-b2ec-603b6d9ec0c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tickers_bbg = ['IBOXHY INDEX', 'IBOXIG INDEX']\n",
    "fields_bbg = ['PX_LAST', 'CONTRBTD_ZSPREAD', 'OAS_SOVEREIGN_CURVE', 'OPTION_ADJ_DURATION_SOV_CRV', 'YIELD_TO_MATURITY']\n",
    "start_date_bbg = \"2000-01-01\"\n",
    "end_date_bbg = datetime.now().strftime('%Y-%m-%d')\n",
    "df_bbg = blp.bdh(tickers=tickers_bbg, flds=fields_bbg, start_date=start_date_bbg, end_date=end_date_bbg)\n",
    "list1_bbg = ['TR', 'Z-Sprd', 'OAS', 'Dur', 'Yield']\n",
    "df_bbg.columns = ['BBG iBoxx HY ' + item for item in list1_bbg] + ['BBG iBoxx IG ' + item for item in list1_bbg] \n",
    "df_bbg.drop(['BBG iBoxx HY Dur', 'BBG iBoxx IG Yield'], axis=1, inplace=True) # First is wrong and second incomplete\n",
    "df_bbg = df_bbg.astype(float)\n",
    "\n",
    "tick = ['USGG5YR INDEX','USGG7YR INDEX','USGG10YR INDEX']\n",
    "all_x = None\n",
    "x = blp.bdh(tickers = tick, flds = 'PX_LAST', start_date = df_bbg.index[0])#, end_date = df_bbg.index[-1])\n",
    "for col in ['BBG']:# + list(set([item.split(' ',1)[0] for item in df_back.columns])):\n",
    "    x1 = x.copy()\n",
    "    x1.columns = [f'{col} US 5Y', f'{col} US 7Y', f'{col} US 10Y']\n",
    "    all_x = pd.concat([all_x,x1],axis=1)\n",
    "df_bbg = pd.concat([df_bbg,all_x],axis=1)\n",
    "\n",
    "tick = ['COA Comdty','VIX Index']\n",
    "x = blp.bdh(tickers = tick, flds='PX_LAST', start_date = df_bbg.index[0])#, end_date = df_bbg.index[-1])\n",
    "x.columns = ['BBG COA Comdty','BBG VIX Index']\n",
    "x = x.sort_index()\n",
    "df_bbg = pd.concat([df_bbg,x],axis=1)\n",
    "df_bbg.index = pd.to_datetime(df_bbg.index)\n",
    "\n",
    "backup = df.copy()\n",
    "df = pd.concat([backup, df_bbg], axis=1).sort_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66854374-5e66-4e93-afdd-3158bda8952a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "options = sorted(backup.columns, key=lambda item: (item.split(' ', 1)[0], 2000 + \\\n",
    "            eval(item.split('due ', 1)[1].split(' ', 1)[0]) if eval(item.split('due ', 1)[1].split(' ', 1)[0])\\\n",
    "                    < 2000 else eval(item.split('due ', 1)[1].split(' ', 1)[0])))\n",
    "\n",
    "options += list(df_bbg.columns)\n",
    "fwd = [item for item in options if 'FWD_' in item]\n",
    "non_fwd = [item for item in options if not('FWD_' in item)]\n",
    "options = non_fwd + fwd\n",
    "\n",
    "time = ['All','2Y','1Y','6M','3M']\n",
    "\n",
    "#################################### Changing order of names\n",
    "o1 = [item for item in options if ((not 'FWD' in item) and ('%' in item))]\n",
    "o2 = [item for item in options if not item in o1]\n",
    "\n",
    "desired_order = [\"Price\", \"G-Sprd\", \"Basis\", \"Duration\", \"CDS\"]\n",
    "\n",
    "def reorder_items_in_chunks(items, order):\n",
    "    chunk_size = len(set([item.rsplit(' ',1)[1] for item in options if ((not 'FWD' in item) and ('%' in item))]))\n",
    "    reordered = []\n",
    "    for i in range(0, len(items), chunk_size):\n",
    "        chunk = items[i:i + chunk_size]\n",
    "        valid_items = [item for item in chunk if len(item.rsplit(' ', 1)) == 2]\n",
    "        reordered_chunk = sorted(valid_items, key=lambda x: order.index(x.rsplit(' ', 1)[1]))\n",
    "        reordered.extend(reordered_chunk)\n",
    "    return reordered\n",
    "\n",
    "o1_reordered = reorder_items_in_chunks(o1, desired_order)\n",
    "options = o1 + o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b06e1124-dd64-46be-8fef-f3fcd3eb8ea2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "############ Default bonds\n",
    "# x = df6[[\"Name\",\"ISIN\",\"Eqty Name\"]].copy()\n",
    "# issue = blp.bdp(tickers=[f\"/isin/{item}@BGN\" for item in x[\"ISIN\"]], flds=\"ISSUE_DT\")\n",
    "# issue1 = issue.copy()\n",
    "# issue1.index = issue1.index.str.replace(\"/isin/\",\"\").str.replace(\"@BGN\",\"\")\n",
    "# issue1.index.name = \"ISIN\"\n",
    "# issue1 = issue1.reset_index()\n",
    "# x = pd.merge(left=x, right=issue1, on=\"ISIN\", how=\"outer\")\n",
    "# x[\"Age\"] = round((datetime.now() - pd.to_datetime(x[\"issue_dt\"])).dt.days/365,2)\n",
    "# x = x[[\"Name\",\"Eqty Name\",\"Age\"]].copy()\n",
    "# x = x.sort_values(by=[\"Eqty Name\",\"Age\"], ascending=[True, False]).reset_index(drop=True).copy()\n",
    "# x = x.drop_duplicates(subset=\"Eqty Name\", keep=\"first\")[[\"Name\"]].copy()\n",
    "# x[\"Name\"] = x[\"Name\"].apply(lambda x: x.rsplit(\" \",1)[0] + \"% due \" + x.rsplit(\"/\",1)[-1])\n",
    "# x[\"MMC\"] = x[\"Name\"].apply(lambda x: [f\"{x.split(\" \",1)[1]} Mat. Matched CDS\", f\"{x.split(\" \",1)[1]} Basis\"])\n",
    "# x[\"Name\"] = x[\"Name\"].apply(lambda x: f\"{x.split(\" \",1)[0]}\")\n",
    "# dict(zip(x[\"Name\"], x[\"MMC\"]))\n",
    "\n",
    "\n",
    "\n",
    "default_values_dict = \\\n",
    "{'TOL': ['3.8% due 29 Mat. Matched CDS', '3.8% due 29 Basis'],\n",
    " 'ZIGGO': ['5 ⅛% due 30 Mat. Matched CDS', '5 ⅛% due 30 Basis'],\n",
    " 'CAR': ['8% due 31 Mat. Matched CDS', '8% due 31 Basis'],\n",
    " 'WFRD': ['8 ⅝% due 30 Mat. Matched CDS', '8 ⅝% due 30 Basis'],\n",
    " 'MPW': ['3 ½% due 31 Mat. Matched CDS', '3 ½% due 31 Basis'],\n",
    " 'OI': ['4 ¾% due 30 Mat. Matched CDS', '4 ¾% due 30 Basis'],\n",
    " 'CHTR': ['4 ¾% due 30 Mat. Matched CDS', '4 ¾% due 30 Basis'],\n",
    " 'HOUS': ['5 ¼% due 30 Mat. Matched CDS', '5 ¼% due 30 Basis'],\n",
    " 'SIRI': ['4 ⅛% due 30 Mat. Matched CDS', '4 ⅛% due 30 Basis'],\n",
    " 'TELEFO': ['8 ¼% due 30 Mat. Matched CDS', '8 ¼% due 30 Basis'],\n",
    " 'SIEGR': ['2.15% due 31 Mat. Matched CDS', '2.15% due 31 Basis'],\n",
    " 'CSCHLD': ['5 ¾% due 30 Mat. Matched CDS', '5 ¾% due 30 Basis'],\n",
    " 'SPG': ['2.45% due 29 Mat. Matched CDS', '2.45% due 29 Basis'],\n",
    " 'RGCARE': ['10% due 32 Mat. Matched CDS', '10% due 32 Basis'],\n",
    " 'TEVA': ['8 ⅛% due 31 Mat. Matched CDS', '8 ⅛% due 31 Basis'],\n",
    " 'HLT': ['4 ⅞% due 30 Mat. Matched CDS', '4 ⅞% due 30 Basis'],\n",
    " 'VST': ['7 ¾% due 31 Mat. Matched CDS', '7 ¾% due 31 Basis'],\n",
    " 'ALIANT': ['5 ⅞% due 29 Mat. Matched CDS', '5 ⅞% due 29 Basis'],\n",
    " 'AXL': ['5% due 29 Mat. Matched CDS', '5% due 29 Basis'],\n",
    " 'IMBLN': ['5 ½% due 30 Mat. Matched CDS', '5 ½% due 30 Basis'],\n",
    " 'DGELN': ['2 ⅜% due 29 Mat. Matched CDS', '2 ⅜% due 29 Basis'],\n",
    " 'TMUS': ['2 ⅞% due 31 Mat. Matched CDS', '2 ⅞% due 31 Basis'],\n",
    " 'URI': ['5 ¼% due 30 Mat. Matched CDS', '5 ¼% due 30 Basis'],\n",
    " 'DOW': ['7 ⅜% due 29 Mat. Matched CDS', '7 ⅜% due 29 Basis'],\n",
    " 'M': ['4 ½% due 34 Mat. Matched CDS', '4 ½% due 34 Basis'],\n",
    " 'JCI': ['1 ¾% due 30 Mat. Matched CDS', '1 ¾% due 30 Basis'],\n",
    " 'AALLN': ['2 ⅞% due 31 Mat. Matched CDS', '2 ⅞% due 31 Basis'],\n",
    " 'OGN': ['5 ⅛% due 31 Mat. Matched CDS', '5 ⅛% due 31 Basis'],\n",
    " 'MEDIND': ['5 ¼% due 29 Mat. Matched CDS', '5 ¼% due 29 Basis'],\n",
    " 'TTEFP': ['5.15% due 34 Mat. Matched CDS', '5.15% due 34 Basis'],\n",
    " 'PRUFIN': ['3 ⅛% due 30 Mat. Matched CDS', '3 ⅛% due 30 Basis'],\n",
    " 'NRUC': ['8% due 32 Mat. Matched CDS', '8% due 32 Basis'],\n",
    " 'LINTA': ['4% due 29 Mat. Matched CDS', '4% due 29 Basis'],\n",
    " 'RIG': ['7 ½% due 31 Mat. Matched CDS', '7 ½% due 31 Basis'],\n",
    " 'HILCRP': ['6% due 31 Mat. Matched CDS', '6% due 31 Basis'],\n",
    " 'CE': ['6.629% due 32 Mat. Matched CDS', '6.629% due 32 Basis'],\n",
    " 'HST': ['3 ⅜% due 29 Mat. Matched CDS', '3 ⅜% due 29 Basis'],\n",
    " 'TRPCN': ['5.6% due 34 Mat. Matched CDS', '5.6% due 34 Basis'],\n",
    " 'BRITEL': ['9 ⅝% due 30 Mat. Matched CDS', '9 ⅝% due 30 Basis'],\n",
    " 'RDSALN': ['2 ⅜% due 29 Mat. Matched CDS', '2 ⅜% due 29 Basis'],\n",
    " 'BMCAUS': ['4 ⅜% due 30 Mat. Matched CDS', '4 ⅜% due 30 Basis'],\n",
    " 'NEE': ['2 ¾% due 29 Mat. Matched CDS', '2 ¾% due 29 Basis'],\n",
    " 'HCA': ['3 ½% due 30 Mat. Matched CDS', '3 ½% due 30 Basis'],\n",
    " 'NESNVX': ['1 ¼% due 30 Mat. Matched CDS', '1 ¼% due 30 Basis'],\n",
    " 'AIFP': ['2 ¼% due 29 Mat. Matched CDS', '2 ¼% due 29 Basis'],\n",
    " 'AEP': ['2.3% due 30 Mat. Matched CDS', '2.3% due 30 Basis'],\n",
    " 'AES': ['3.95% due 30 Mat. Matched CDS', '3.95% due 30 Basis'],\n",
    " 'OMF': ['5 ⅜% due 29 Mat. Matched CDS', '5 ⅜% due 29 Basis'],\n",
    " 'AIG': ['3 ⅞% due 35 Mat. Matched CDS', '3 ⅞% due 35 Basis'],\n",
    " 'ALL': ['5.35% due 33 Mat. Matched CDS', '5.35% due 33 Basis'],\n",
    " 'ALLY': ['8% due 31 Mat. Matched CDS', '8% due 31 Basis'],\n",
    " 'AMGN': ['2.45% due 30 Mat. Matched CDS', '2.45% due 30 Basis'],\n",
    " 'AMZN': ['4.8% due 34 Mat. Matched CDS', '4.8% due 34 Basis'],\n",
    " 'APA': ['4 ¼% due 30 Mat. Matched CDS', '4 ¼% due 30 Basis'],\n",
    " 'ARW': ['2.95% due 32 Mat. Matched CDS', '2.95% due 32 Basis'],\n",
    " 'AVGO': ['5% due 30 Mat. Matched CDS', '5% due 30 Basis'],\n",
    " 'AVT': ['3% due 31 Mat. Matched CDS', '3% due 31 Basis'],\n",
    " 'AXP': ['4.42% due 33 Mat. Matched CDS', '4.42% due 33 Basis'],\n",
    " 'AZN': ['1 ⅜% due 30 Mat. Matched CDS', '1 ⅜% due 30 Basis'],\n",
    " 'AZO': ['4% due 30 Mat. Matched CDS', '4% due 30 Basis'],\n",
    " 'BA': ['6 ⅛% due 33 Mat. Matched CDS', '6 ⅛% due 33 Basis'],\n",
    " 'BALN': ['3.4% due 30 Mat. Matched CDS', '3.4% due 30 Basis'],\n",
    " 'BACR': ['2.645% due 31 Mat. Matched CDS', '2.645% due 31 Basis'],\n",
    " 'BAX': ['3.95% due 30 Mat. Matched CDS', '3.95% due 30 Basis'],\n",
    " 'BBY': ['1.95% due 30 Mat. Matched CDS', '1.95% due 30 Basis'],\n",
    " 'BMY': ['1.45% due 30 Mat. Matched CDS', '1.45% due 30 Basis'],\n",
    " 'BRK': ['6 ½% due 34 Mat. Matched CDS', '6 ½% due 34 Basis'],\n",
    " 'BSX': ['2.65% due 30 Mat. Matched CDS', '2.65% due 30 Basis'],\n",
    " 'CAG': ['8 ¼% due 30 Mat. Matched CDS', '8 ¼% due 30 Basis'],\n",
    " 'CAH': ['5.45% due 34 Mat. Matched CDS', '5.45% due 34 Basis'],\n",
    " 'CMCSA': ['7.05% due 33 Mat. Matched CDS', '7.05% due 33 Basis'],\n",
    " 'CNQCN': ['7.2% due 32 Mat. Matched CDS', '7.2% due 32 Basis'],\n",
    " 'COF': ['2.7% due 30 Mat. Matched CDS', '2.7% due 30 Basis'],\n",
    " 'COXENT': ['4.8% due 35 Mat. Matched CDS', '4.8% due 35 Basis'],\n",
    " 'CPB': ['2 ⅜% due 30 Mat. Matched CDS', '2 ⅜% due 30 Basis'],\n",
    " 'CSCO': ['4.95% due 31 Mat. Matched CDS', '4.95% due 31 Basis'],\n",
    " 'CSX': ['2.4% due 30 Mat. Matched CDS', '2.4% due 30 Basis'],\n",
    " 'CVS': ['4 ⅞% due 35 Mat. Matched CDS', '4 ⅞% due 35 Basis'],\n",
    " 'D': ['6.3% due 33 Mat. Matched CDS', '6.3% due 33 Basis'],\n",
    " 'DAL': ['3 ¾% due 29 Mat. Matched CDS', '3 ¾% due 29 Basis'],\n",
    " 'DE': ['7 ⅛% due 31 Mat. Matched CDS', '7 ⅛% due 31 Basis'],\n",
    " 'DGX': ['2.95% due 30 Mat. Matched CDS', '2.95% due 30 Basis'],\n",
    " 'DHI': ['5% due 34 Mat. Matched CDS', '5% due 34 Basis'],\n",
    " 'DHR': ['2.6% due 29 Mat. Matched CDS', '2.6% due 29 Basis'],\n",
    " 'DIS': ['7% due 32 Mat. Matched CDS', '7% due 32 Basis'],\n",
    " 'DRI': ['6.3% due 33 Mat. Matched CDS', '6.3% due 33 Basis'],\n",
    " 'DT': ['8 ¼% due 30 Mat. Matched CDS', '8 ¼% due 30 Basis'],\n",
    " 'DVN': ['7 ⅞% due 31 Mat. Matched CDS', '7 ⅞% due 31 Basis'],\n",
    " 'EDF': ['6 ¼% due 33 Mat. Matched CDS', '6 ¼% due 33 Basis'],\n",
    " 'EMN': ['5 ¾% due 33 Mat. Matched CDS', '5 ¾% due 33 Basis'],\n",
    " 'ENBCN': ['3 ⅛% due 29 Mat. Matched CDS', '3 ⅛% due 29 Basis'],\n",
    " 'ENGIFP': ['5 ⅝% due 34 Mat. Matched CDS', '5 ⅝% due 34 Basis'],\n",
    " 'ENIIM': ['5 ½% due 34 Mat. Matched CDS', '5 ½% due 34 Basis'],\n",
    " 'EQNR': ['3 ⅛% due 30 Mat. Matched CDS', '3 ⅛% due 30 Basis'],\n",
    " 'ET': ['4.9% due 35 Mat. Matched CDS', '4.9% due 35 Basis'],\n",
    " 'EXC': ['5 ⅝% due 35 Mat. Matched CDS', '5 ⅝% due 35 Basis'],\n",
    " 'EXPE': ['3 ¼% due 30 Mat. Matched CDS', '3 ¼% due 30 Basis'],\n",
    " 'F': ['7.45% due 31 Mat. Matched CDS', '7.45% due 31 Basis'],\n",
    " 'FCX': ['5.4% due 34 Mat. Matched CDS', '5.4% due 34 Basis'],\n",
    " 'FDX': ['4 ¼% due 30 Mat. Matched CDS', '4 ¼% due 30 Basis'],\n",
    " 'FE': ['4.55% due 30 Mat. Matched CDS', '4.55% due 30 Basis'],\n",
    " 'GE': ['6 ¾% due 32 Mat. Matched CDS', '6 ¾% due 32 Basis'],\n",
    " 'GIS': ['2 ⅞% due 30 Mat. Matched CDS', '2 ⅞% due 30 Basis'],\n",
    " 'GM': ['5% due 35 Mat. Matched CDS', '5% due 35 Basis'],\n",
    " 'HAL': ['2.92% due 30 Mat. Matched CDS', '2.92% due 30 Basis'],\n",
    " 'HD': ['2.7% due 30 Mat. Matched CDS', '2.7% due 30 Basis'],\n",
    " 'HES': ['7 ⅞% due 29 Mat. Matched CDS', '7 ⅞% due 29 Basis'],\n",
    " 'KHC': ['6 ¾% due 32 Mat. Matched CDS', '6 ¾% due 32 Basis'],\n",
    " 'HON': ['1.95% due 30 Mat. Matched CDS', '1.95% due 30 Basis'],\n",
    " 'HPQ': ['3.4% due 30 Mat. Matched CDS', '3.4% due 30 Basis'],\n",
    " 'HSBC': ['3.973% due 30 Mat. Matched CDS', '3.973% due 30 Basis'],\n",
    " 'IBM': ['5 ⅞% due 32 Mat. Matched CDS', '5 ⅞% due 32 Basis'],\n",
    " 'INTNED': ['2.727% due 32 Mat. Matched CDS', '2.727% due 32 Basis'],\n",
    " 'INTC': ['4% due 32 Mat. Matched CDS', '4% due 32 Basis'],\n",
    " 'JNJ': ['6.95% due 29 Mat. Matched CDS', '6.95% due 29 Basis'],\n",
    " 'KMI': ['7.4% due 31 Mat. Matched CDS', '7.4% due 31 Basis'],\n",
    " 'KPN': ['8 ⅜% due 30 Mat. Matched CDS', '8 ⅜% due 30 Basis'],\n",
    " 'KR': ['7 ½% due 31 Mat. Matched CDS', '7 ½% due 31 Basis'],\n",
    " 'L': ['6% due 35 Mat. Matched CDS', '6% due 35 Basis'],\n",
    " 'LEN': ['5.2% due 30 Mat. Matched CDS', '5.2% due 30 Basis'],\n",
    " 'LLOYDS': ['0% due 32 Mat. Matched CDS', '0% due 32 Basis'],\n",
    " 'LMT': ['3.6% due 35 Mat. Matched CDS', '3.6% due 35 Basis'],\n",
    " 'LNC': ['3.05% due 30 Mat. Matched CDS', '3.05% due 30 Basis'],\n",
    " 'LOW': ['4 ½% due 30 Mat. Matched CDS', '4 ½% due 30 Basis'],\n",
    " 'LUV': ['2 ⅝% due 30 Mat. Matched CDS', '2 ⅝% due 30 Basis'],\n",
    " 'MCD': ['2 ⅝% due 29 Mat. Matched CDS', '2 ⅝% due 29 Basis'],\n",
    " 'MCK': ['5.1% due 33 Mat. Matched CDS', '5.1% due 33 Basis'],\n",
    " 'MDC': ['3.85% due 30 Mat. Matched CDS', '3.85% due 30 Basis'],\n",
    " 'MDLZ': ['2 ¾% due 30 Mat. Matched CDS', '2 ¾% due 30 Basis'],\n",
    " 'MET': ['6 ½% due 32 Mat. Matched CDS', '6 ½% due 32 Basis'],\n",
    " 'MO': ['3.4% due 30 Mat. Matched CDS', '3.4% due 30 Basis'],\n",
    " 'MPC': ['5.15% due 30 Mat. Matched CDS', '5.15% due 30 Basis'],\n",
    " 'MSI': ['2.3% due 30 Mat. Matched CDS', '2.3% due 30 Basis'],\n",
    " 'MTNA': ['6.8% due 32 Mat. Matched CDS', '6.8% due 32 Basis'],\n",
    " 'NEM': ['5 ⅞% due 35 Mat. Matched CDS', '5 ⅞% due 35 Basis'],\n",
    " 'NFLX': ['5 ⅜% due 29 Mat. Matched CDS', '5 ⅜% due 29 Basis'],\n",
    " 'NGGLN': ['5.809% due 33 Mat. Matched CDS', '5.809% due 33 Basis'],\n",
    " 'NOC': ['4.4% due 30 Mat. Matched CDS', '4.4% due 30 Basis'],\n",
    " 'NSC': ['2.55% due 29 Mat. Matched CDS', '2.55% due 29 Basis'],\n",
    " 'NWG': ['5.076% due 30 Mat. Matched CDS', '5.076% due 30 Basis'],\n",
    " 'OMC': ['2.45% due 30 Mat. Matched CDS', '2.45% due 30 Basis'],\n",
    " 'ORAFP': ['8 ½% due 31 Mat. Matched CDS', '8 ½% due 31 Basis'],\n",
    " 'ORCL': ['4.3% due 34 Mat. Matched CDS', '4.3% due 34 Basis'],\n",
    " 'OVV': ['8 ⅛% due 30 Mat. Matched CDS', '8 ⅛% due 30 Basis'],\n",
    " 'OXY': ['7 ½% due 31 Mat. Matched CDS', '7 ½% due 31 Basis'],\n",
    " 'PARA': ['7 ⅞% due 30 Mat. Matched CDS', '7 ⅞% due 30 Basis'],\n",
    " 'PFE': ['6 ½% due 34 Mat. Matched CDS', '6 ½% due 34 Basis'],\n",
    " 'PG': ['5.8% due 34 Mat. Matched CDS', '5.8% due 34 Basis'],\n",
    " 'PHM': ['7 ⅞% due 32 Mat. Matched CDS', '7 ⅞% due 32 Basis'],\n",
    " 'PKG': ['3% due 29 Mat. Matched CDS', '3% due 29 Basis'],\n",
    " 'PRU': ['5 ¾% due 33 Mat. Matched CDS', '5 ¾% due 33 Basis'],\n",
    " 'R': ['6.6% due 33 Mat. Matched CDS', '6.6% due 33 Basis'],\n",
    " 'RIFP': ['1 ⅝% due 31 Mat. Matched CDS', '1 ⅝% due 31 Basis'],\n",
    " 'RTX': ['7 ½% due 29 Mat. Matched CDS', '7 ½% due 29 Basis'],\n",
    " 'SHW': ['2.3% due 30 Mat. Matched CDS', '2.3% due 30 Basis'],\n",
    " 'SO': ['2.65% due 29 Mat. Matched CDS', '2.65% due 29 Basis'],\n",
    " 'SRE': ['5 ½% due 33 Mat. Matched CDS', '5 ½% due 33 Basis'],\n",
    " 'STANLN': ['4.305% due 30 Mat. Matched CDS', '4.305% due 30 Basis'],\n",
    " 'T': ['4 ½% due 35 Mat. Matched CDS', '4 ½% due 35 Basis'],\n",
    " 'TGT': ['6.35% due 32 Mat. Matched CDS', '6.35% due 32 Basis'],\n",
    " 'TSN': ['4 ⅞% due 34 Mat. Matched CDS', '4 ⅞% due 34 Basis'],\n",
    " 'UBS': ['3.126% due 30 Mat. Matched CDS', '3.126% due 30 Basis'],\n",
    " 'UNH': ['4 ⅝% due 35 Mat. Matched CDS', '4 ⅝% due 35 Basis'],\n",
    " 'UNP': ['3 ⅜% due 35 Mat. Matched CDS', '3 ⅜% due 35 Basis'],\n",
    " 'VLO': ['7 ½% due 32 Mat. Matched CDS', '7 ½% due 32 Basis'],\n",
    " 'VZ': ['7 ¾% due 30 Mat. Matched CDS', '7 ¾% due 30 Basis'],\n",
    " 'WHR': ['2.4% due 31 Mat. Matched CDS', '2.4% due 31 Basis'],\n",
    " 'WMB': ['7 ½% due 31 Mat. Matched CDS', '7 ½% due 31 Basis'],\n",
    " 'WMT': ['7.55% due 30 Mat. Matched CDS', '7.55% due 30 Basis'],\n",
    " 'WY': ['7 ⅜% due 32 Mat. Matched CDS', '7 ⅜% due 32 Basis']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f93c4554-4e53-41a1-a886-ee07442f18fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b148f500634710b4f0c8c66c0956d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Issuer', options=('All', 'AALLN', 'AEP', 'AES', 'AIFP', 'A…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca33eba6c294e36a177e6bf7708867c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='a', description='Horizontal Axis:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694ae7ab9a9444dfb6ea36539d0f38f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='b', description='Vertical Axis:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b04ffea7254ac4ac2c0701d2142d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46cf93dc29d4d4c88f8e234bd4b759a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################################### Changing order of names\n",
    "\n",
    "issuer = ['All'] + list(sorted(set([item.split(' ',1)[0] for item in options])))\n",
    "options1 = list(set([item.split(' ',1)[1] for item in options])) + ['US 5Y','US 7Y','US 10Y']\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "\n",
    "dropdown1 = Dropdown(options=issuer, description='Issuer')\n",
    "dropdown2 = Dropdown(options=options1, description='A:')\n",
    "dropdown3 = Dropdown(options=options1, description='B:')\n",
    "dropdown4 = Dropdown(options=options1, description='C:')\n",
    "dropdown5 = Dropdown(options=options1, description='D:')\n",
    "dropdown6 = Dropdown(options=options1, description='E:')\n",
    "dropdown7 = Dropdown(options=options1, description='F:')\n",
    "dropdown8 = Dropdown(options=options1, description='G:')\n",
    "dropdown9 = Dropdown(options=options1, description='H:')\n",
    "dropdown10 = Dropdown(options=options1, description='I:')\n",
    "dropdown11 = Dropdown(options=options1, description='J:')\n",
    "dropdown12 = Dropdown(options=options1, description='K:')\n",
    "dropdown13 = Dropdown(options=options1, description='L:')\n",
    "# dropdown14 = Dropdown(options=time, description='Time:')\n",
    "\n",
    "dropdown14 = widgets.FloatText(\n",
    "    value=5,\n",
    "    description='Years:',\n",
    "    step=0.01\n",
    ")\n",
    "\n",
    "row1 = HBox([dropdown1])\n",
    "row2 = HBox([dropdown2, dropdown3, dropdown4])\n",
    "row3 = HBox([dropdown5, dropdown6, dropdown7])\n",
    "row4 = HBox([dropdown8, dropdown9, dropdown10])\n",
    "row5 = HBox([dropdown11, dropdown12, dropdown13])\n",
    "row6 = HBox([dropdown14])\n",
    "\n",
    "dropdown_box = VBox([row1, row2, row3, row4, row5, row6])\n",
    "\n",
    "display(dropdown_box)\n",
    "\n",
    "expression_input1 = Text(description='Horizontal Axis:',value='a')\n",
    "expression_input2 = Text(description='Vertical Axis:',value='b')\n",
    "display(expression_input1)\n",
    "display(expression_input2)\n",
    "\n",
    "def get_values():\n",
    "    return dropdown1.value, dropdown2.value, dropdown3.value, \\\n",
    "    dropdown4.value, dropdown5.value, dropdown6.value, \\\n",
    "    dropdown7.value, dropdown8.value, dropdown9.value, \\\n",
    "    dropdown10.value, dropdown11.value, dropdown12.value, \\\n",
    "    dropdown13.value, dropdown14.value\n",
    "\n",
    "submit_button = Button(description=\"Submit\")\n",
    "display(submit_button)\n",
    "\n",
    "output = Output()\n",
    "display(output)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        global df\n",
    "        clear_output()\n",
    "        selected_values = get_values()\n",
    "        x = list(selected_values)\n",
    "        if x[0] != 'All':\n",
    "            x = [x[0]] + [f\"{x[0]} {item}\" for item in x[1:-1]] + [x[-1]]\n",
    "            selected_values = [x[0]] + \\\n",
    "            ['BBG ' + item.split(' ',1)[1] if (len(item.split(' '))>2 and item.split(' ')[1]=='US') else item for item in x[1:-1]]\\\n",
    "            +[x[-1]]\n",
    "            selected_values = tuple(selected_values)\n",
    "        \n",
    "        expression1 = expression_input1.value.upper()\n",
    "        expression2 = expression_input2.value.upper()\n",
    "        \n",
    "        col_map = {'A': selected_values[1], 'B': selected_values[2], 'C': selected_values[3], \n",
    "                   'D': selected_values[4], 'E': selected_values[5], 'F': selected_values[6],\n",
    "                   'G': selected_values[7], 'H': selected_values[8], 'I': selected_values[9], \n",
    "                   'J': selected_values[10], 'K': selected_values[11], 'L': selected_values[12]}\n",
    "        \n",
    "        for key, value in col_map.items():\n",
    "            expression1 = expression1.replace(key, key.lower())\n",
    "            expression2 = expression2.replace(key, key.lower())\n",
    "        \n",
    "        a, b, c, d, e, f, g, h, i, j, k, l = sp.symbols('a b c d e f g h i j k l')\n",
    "        expr1 = sp.sympify(expression1)\n",
    "        expr11 = sp.sympify(expression1)\n",
    "        expr2 = sp.sympify(expression2) if expression2 else None\n",
    "        expr21 = sp.sympify(expression2) if expression2 else None\n",
    "\n",
    "        cols_to_check = [col_map['A'], col_map['B'], col_map['C'], \\\n",
    "                         col_map['D'], col_map['E'], col_map['F'], \\\n",
    "                         col_map['G'], col_map['H'], col_map['I'], \\\n",
    "                         col_map['J'], col_map['K'], col_map['L'] ]\n",
    "        \n",
    "        df1 = df.copy()\n",
    "        df2 = df.copy()\n",
    "        \n",
    "        replacements = {\n",
    "            'A': selected_values[1], 'a': selected_values[1],\n",
    "            'B': selected_values[2], 'b': selected_values[2],\n",
    "            'C': selected_values[3], 'c': selected_values[3],\n",
    "            'D': selected_values[4], 'd': selected_values[4],\n",
    "            'E': selected_values[5], 'e': selected_values[5],\n",
    "            'F': selected_values[6], 'f': selected_values[6],\n",
    "            'G': selected_values[7], 'g': selected_values[7],\n",
    "            'H': selected_values[8], 'h': selected_values[8],\n",
    "            'I': selected_values[9], 'i': selected_values[9],\n",
    "            'J': selected_values[10], 'j': selected_values[10],\n",
    "            'K': selected_values[11], 'k': selected_values[11],\n",
    "            'L': selected_values[12], 'l': selected_values[12]\n",
    "        }\n",
    "\n",
    "        regex = re.compile(\"|\".join(re.escape(key) for key in replacements.keys()))\n",
    "        used_keys = set()\n",
    "        def substitution(match):\n",
    "            key = match.group(0)\n",
    "            used_keys.add(key)\n",
    "            return replacements[key]\n",
    "        \n",
    "        expr111 = regex.sub(substitution, str(expr11))\n",
    "        expr211 = regex.sub(substitution, str(expr21))\n",
    "        used_substitutions = list(used_keys)\n",
    "\n",
    "        used_substitutions = [replacements[item.upper()] for item in used_substitutions]\n",
    "\n",
    "        df1['Expression 1'] = df1.apply(lambda row: expr1.evalf(subs={\\\n",
    "            a: row[col_map['A']], b: row[col_map['B']], \n",
    "            c: row[col_map['C']], d: row[col_map['D']], \n",
    "            e: row[col_map['E']], f: row[col_map['F']],\n",
    "            g: row[col_map['G']], h: row[col_map['H']],\n",
    "            i: row[col_map['I']], j: row[col_map['J']], \n",
    "            k: row[col_map['K']], l: row[col_map['L']] }), axis=1).astype(float)       \n",
    "        df1['Expression 2'] = df1.apply(lambda row: expr2.evalf(subs={\\\n",
    "            a: row[col_map['A']], b: row[col_map['B']], \n",
    "            c: row[col_map['C']], d: row[col_map['D']], \n",
    "            e: row[col_map['E']], f: row[col_map['F']],\n",
    "            g: row[col_map['G']], h: row[col_map['H']],\n",
    "            i: row[col_map['I']], j: row[col_map['J']], \n",
    "            k: row[col_map['K']], l: row[col_map['L']] }) if expr2 else np.nan, axis=1).astype(float)\n",
    "\n",
    "        plot_exp1 = np.nan\n",
    "        plot_exp2 = np.nan\n",
    "        \n",
    "        if ('FWD' in expr111) and not ('FWD' in expr211) and expression2:\n",
    "            df2['Expression 2'] = df2.apply(lambda row: expr2.evalf(subs={\\\n",
    "            a: row[col_map['A']], b: row[col_map['B']], \n",
    "            c: row[col_map['C']], d: row[col_map['D']], \n",
    "            e: row[col_map['E']], f: row[col_map['F']],\n",
    "            g: row[col_map['G']], h: row[col_map['H']],\n",
    "            i: row[col_map['I']], j: row[col_map['J']], \n",
    "            k: row[col_map['K']], l: row[col_map['L']] }), axis=1).astype(float)\n",
    "            plot_exp2 = df2.iloc[-1]['Expression 2'].astype(float)\n",
    "        if ('FWD' in expr211) and not ('FWD' in expr111):\n",
    "            df2['Expression 1'] = df2.apply(lambda row: expr1.evalf(subs={\\\n",
    "            a: row[col_map['A']], b: row[col_map['B']], \n",
    "            c: row[col_map['C']], d: row[col_map['D']], \n",
    "            e: row[col_map['E']], f: row[col_map['F']],\n",
    "            g: row[col_map['G']], h: row[col_map['H']],\n",
    "            i: row[col_map['I']], j: row[col_map['J']], \n",
    "            k: row[col_map['K']], l: row[col_map['L']] }), axis=1).astype(float)\n",
    "            plot_exp1 = df2.iloc[-1]['Expression 1'].astype(float)\n",
    "\n",
    "        \n",
    "        def filter_df(dfx1, time_period):\n",
    "            dfx = dfx1.copy()\n",
    "            dfx.index=pd.to_datetime(dfx.index)\n",
    "            if time_period == 'All':\n",
    "                return dfx\n",
    "            else:\n",
    "                now = datetime.now()\n",
    "                # if time_period == '2Y':\n",
    "                #     start_date = now - timedelta(days=2*365)\n",
    "                # elif time_period == '1Y':\n",
    "                #     start_date = now - timedelta(days=365)\n",
    "                # elif time_period == '6M':\n",
    "                #     start_date = now - timedelta(days=6*30)\n",
    "                # elif time_period == '3M':\n",
    "                #     start_date = now - timedelta(days=3*30)\n",
    "                # return dfx[dfx.index.date >= start_date.date()]   \n",
    "                \n",
    "                start_date = now - timedelta(days=int(time_period*365))\n",
    "                return dfx[dfx.index.date >= start_date.date()] \n",
    "\n",
    "        def update_plot(plot_exp1,plot_exp2,df1,df2):\n",
    "            df1 = df1[used_substitutions + ['Expression 1','Expression 2']].dropna().copy()\n",
    "            # display(df1)\n",
    "            if expression2:\n",
    "                fig = make_subplots(rows=2, cols=1, shared_xaxes=False, \\\n",
    "                                    vertical_spacing=0.1, specs=[ [{\"secondary_y\":False}], [{\"secondary_y\":True}] ],\\\n",
    "                                   subplot_titles=(f'Scatter Plot from {df1.index[0].date()} to {df1.index[-1].date()}',\\\n",
    "                                                  f'{expr111}: {df1[df1.columns[-2]][-1]:.4f}   &   {expr211}: {df1[df1.columns[-1]][-1]:.4f}'))\n",
    "                \n",
    "                x_scatter = df1[df1.columns[-2]]\n",
    "                y_scatter = df1[df1.columns[-1]]\n",
    "\n",
    "                coeffs = np.polyfit(x_scatter,y_scatter,2)\n",
    "                polynomial = np.poly1d(coeffs)\n",
    "                x_poly=np.linspace(min(x_scatter),max(x_scatter),500)\n",
    "                y_poly=polynomial(x_poly)\n",
    "                y_pred = polynomial(x_scatter)\n",
    "                r2=r2_score(y_scatter,y_pred)\n",
    "                equation_text = f\"y = {coeffs[0]:.4f}*x² + {coeffs[1]:.4f}*x + {coeffs[2]:.4f}\"\n",
    "    \n",
    "                residuals = y_scatter-y_pred\n",
    "                res_sum=np.sum(residuals**2)\n",
    "                dof=len(x_scatter)-len(coeffs)\n",
    "                res_var = res_sum / dof\n",
    "                se = np.sqrt(res_var)\n",
    "                conf_upper = y_poly + 2 * se\n",
    "                conf_lower = y_poly - 2 * se\n",
    "                \n",
    "                fig.add_trace(go.Scatter(x=df1[df1.columns[-2]][-63:-1], y=df1[df1.columns[-1]][-63:-1], \\\n",
    "                                         name='< 3 months', mode='markers',marker=dict(color='green')), \\\n",
    "                              row=1,col=1,secondary_y=False)\n",
    "                fig.add_trace(go.Scatter(x=df1[df1.columns[-2]][-252:-63], y=df1[df1.columns[-1]][-252:-63], \\\n",
    "                                         name='> 3 months & < 1 year', mode='markers',marker=dict(color='blue')), \\\n",
    "                              row=1,col=1,secondary_y=False)\n",
    "                fig.add_trace(go.Scatter(x=df1[df1.columns[-2]][:-252], y=df1[df1.columns[-1]][:-252], \\\n",
    "                                         name='> 1 year', mode='markers',marker=dict(color='gray')), \\\n",
    "                              row=1,col=1,secondary_y=False)\n",
    "                \n",
    "                fig.add_trace(go.Scatter(x=[df1[df1.columns[-2]].iloc[-1]], y=[df1[df1.columns[-1]].iloc[-1]], \\\n",
    "                                         name=f'Value as of {df1.index[-1].date()}', mode='markers',marker=dict(color='red',size=10),\\\n",
    "                                         showlegend=True), row=1,col=1,secondary_y=False)      \n",
    "                fig.add_trace(go.Scatter(x=x_poly, y=y_poly, \\\n",
    "                                         name=f'Polynomial Fit<br>R² = {r2:.3f}<br>{equation_text}', \\\n",
    "                                         mode='lines', line=dict(color='orange'),\n",
    "                                         showlegend=True), row=1,col=1,secondary_y=False) \n",
    "               \n",
    "                fig.add_trace(go.Scatter(x=x_poly, y=conf_upper, \\\n",
    "                                         name='Upper Confidence Band', \\\n",
    "                                         mode='lines', line=dict(color='orange',dash='dash'),\n",
    "                                         showlegend=True), row=1,col=1,secondary_y=False) \n",
    "                fig.add_trace(go.Scatter(x=x_poly, y=conf_lower, \\\n",
    "                                         name='Lower Confidence Band', fill='tonexty',\\\n",
    "                                         fillcolor='rgba(255,165,0,0.15)',\n",
    "                                         mode='lines', line=dict(color='orange',dash='dash'),\n",
    "                                         showlegend=True), row=1,col=1,secondary_y=False) \n",
    "                \n",
    "                if ('FWD' in expr111) and not ('FWD' in expr211):\n",
    "                    # display(f\"Condition1\")\n",
    "                    plot_exp2 = [plot_exp2] * len(df1[df1.columns[-2]])\n",
    "                    fig.add_trace(go.Scatter(x=df1[df1.columns[-2]], y=plot_exp2, mode='lines',\\\n",
    "                     line=dict(color='red',width=1),name = f'{expr211} on {df2.index[-1]} was {plot_exp2[0]}'), \\\n",
    "                      row=1,col=1,secondary_y=False)\n",
    "\n",
    "                if ('FWD' in expr211) and not ('FWD' in expr111):\n",
    "                    # display(f\"Condition2\")\n",
    "                    plot_exp1 = [plot_exp1] * len(df1[df1.columns[-1]])\n",
    "                    fig.add_trace(go.Scatter(x=plot_exp1, y=df1[df1.columns[-1]], mode='lines',\\\n",
    "                     line=dict(color='red',width=1), name = f'{expr111} on {df2.index[-1]} was {plot_exp1[0]}'), \\\n",
    "                      row=1,col=1,secondary_y=False)\n",
    "                \n",
    "                fig.add_trace(go.Scatter(x=df1.index, y=df1[df1.columns[-2]], name=expr111,\\\n",
    "                         line=dict(color='blue')), row=2,col=1,secondary_y=False)\n",
    "                \n",
    "                fig.add_trace(go.Scatter(x=[df1.index[0], df1.index[-1]], \\\n",
    "                 y=[df1[df1.columns[-2]][-1], df1[df1.columns[-2]][-1]], \\\n",
    "                 mode='lines', line=dict(dash='dash', color='blue'), \\\n",
    "                 name=f'Recent value of {expr111} is {df1[df1.columns[-2]][-1]}'),row=2,col=1,secondary_y=False)\n",
    "                \n",
    "                fig.add_trace(go.Scatter(x=df1.index, y=df1[df1.columns[-1]], \\\n",
    "                                     name=expr211, line=dict(color='green')),\\\n",
    "                                      row=2,col=1, secondary_y=True)\n",
    "                \n",
    "                fig.add_trace(go.Scatter(x=[df1.index[0], df1.index[-1]], \\\n",
    "                 y=[df1[df1.columns[-1]][-1], df1[df1.columns[-1]][-1]], \\\n",
    "                 mode='lines', line=dict(dash='dash', color='green'), \\\n",
    "                 name=f'Recent value of {expr211} is {df1[df1.columns[-1]][-1]}'),row=2,col=1, secondary_y=True)\n",
    "                \n",
    "                fig.update_layout(\n",
    "                hovermode='x unified',\n",
    "                plot_bgcolor='white',\n",
    "                paper_bgcolor='white',\n",
    "                legend=dict(orientation=\"h\",yanchor=\"bottom\",xanchor=\"center\",x=0.5,y=1.05),\n",
    "                height=1900,\n",
    "                width=1100)\n",
    "\n",
    "            else:\n",
    "                fig = make_subplots(rows=1, cols=1, shared_xaxes=False, \\\n",
    "                                    vertical_spacing=0.1, subplot_titles=(f'{expr111}: {df1[df1.columns[-2]][-1]:.4f}'))\n",
    "                \n",
    "                fig.add_trace(go.Scatter(x=df1.index, y=df1[df1.columns[-2]], name=expr111,\\\n",
    "                         line=dict(color='blue')), row=1,col=1)\n",
    "                \n",
    "                fig.add_trace(go.Scatter(x=[df1.index[0], df1.index[-1]], \\\n",
    "                 y=[df1[df1.columns[-2]][-1], df1[df1.columns[-2]][-1]], \\\n",
    "                 mode='lines', line=dict(dash='dash', color='blue'), \\\n",
    "                 name=f'Recent value of {expr111} is {df1[df1.columns[-2]][-1]}'))\n",
    "\n",
    "                fig.update_layout(\n",
    "                yaxis=dict(zeroline=True, zerolinecolor='black'),\n",
    "                hovermode='x unified',\n",
    "                plot_bgcolor='white',\n",
    "                paper_bgcolor='white',\n",
    "                legend=dict(orientation=\"h\",yanchor=\"bottom\",xanchor=\"center\",x=0.5,y=1.05),\n",
    "                height=800,\n",
    "                width=1100)\n",
    "\n",
    "            def split_label(label,n):\n",
    "                return '<br>'.join([label[i:i+n] for i in range(0,len(label),n)])\n",
    "\n",
    "            label1 = split_label(expr111,38)\n",
    "            label2 = split_label(expr211,38)\n",
    "          \n",
    "            if expression2:\n",
    "                fig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True,\\\n",
    "                                 row=1,col=1,title=label1, showgrid=True, gridcolor='LightGrey',gridwidth=1)\n",
    "                fig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True,\\\n",
    "                                 row=1,col=1, title=label2, secondary_y=False, showgrid=True, gridcolor='LightGrey',gridwidth=1)\n",
    "                fig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True, \\\n",
    "                                 title='Date', row=2, col=1, showgrid=True, gridcolor='LightGrey',gridwidth=1)\n",
    "                fig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True, \\\n",
    "                                 title=label1, side='left', row=2,col=1, secondary_y=False, \\\n",
    "                                 showgrid=True, gridcolor='LightGrey',gridwidth=1, title_font = dict(color='blue'))\n",
    "                fig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True, \\\n",
    "                                 title=label2, side='right', row=2,col=1, secondary_y=True, title_font = dict(color='green'))\n",
    "            else:\n",
    "                fig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True, \\\n",
    "                                 title='Date', row=1, col=1, showgrid=True, gridcolor='LightGrey',gridwidth=1)\n",
    "                fig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True, \\\n",
    "                                 title=label1, side='left', row=1,col=1, secondary_y=False, showgrid=True, gridcolor='LightGrey',gridwidth=1)\n",
    "            fig.show()\n",
    "        \n",
    "        df1 = filter_df(df1, selected_values[-1]).copy() #Time goes here\n",
    "        interact(update_plot(plot_exp1,plot_exp2,df1,df2))\n",
    "\n",
    "submit_button.on_click(on_button_clicked)\n",
    "\n",
    "def update_dropdowns(*args):\n",
    "    selected_issuer = dropdown1.value\n",
    "    if selected_issuer:\n",
    "        for i in range(2, 14):\n",
    "            dropdown = globals().get(f'dropdown{i}')\n",
    "            if dropdown:\n",
    "                if selected_issuer != 'All':\n",
    "                    x = list([item.split(' ', 1)[1] for item in options if item.startswith(selected_issuer)]) + ['US 5Y','US 7Y','US 10Y']\n",
    "                else:\n",
    "                    x = options\n",
    "                dropdown.options = x\n",
    "        if selected_issuer in list(default_values_dict.keys()):\n",
    "            globals().get(f'dropdown2').value = default_values_dict[selected_issuer][0]\n",
    "            globals().get(f'dropdown3').value = default_values_dict[selected_issuer][1]\n",
    "\n",
    "dropdown1.observe(update_dropdowns, names='value')\n",
    "update_dropdowns()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
